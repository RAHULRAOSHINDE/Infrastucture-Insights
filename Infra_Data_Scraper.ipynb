{"cells":[{"cell_type":"code","execution_count":null,"id":"10d522a2-eda1-477c-a49d-cbf39ffc5907","metadata":{"id":"10d522a2-eda1-477c-a49d-cbf39ffc5907","tags":[],"outputId":"185bda7e-0333-4ca1-b283-9e46e963d4c3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading package lists... Done\n","E: Could not open lock file /var/lib/apt/lists/lock - open (13: Permission denied)\n","E: Unable to lock directory /var/lib/apt/lists/\n","W: Problem unlinking the file /var/cache/apt/pkgcache.bin - RemoveCaches (13: Permission denied)\n","W: Problem unlinking the file /var/cache/apt/srcpkgcache.bin - RemoveCaches (13: Permission denied)\n","\u001b[1;31mE: \u001b[0mCould not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\u001b[0m\n","\u001b[1;31mE: \u001b[0mUnable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\u001b[0m\n","--2024-02-19 11:21:32--  https://github.com/mozilla/geckodriver/releases/download/v0.34.0/geckodriver-v0.34.0-linux64.tar.gz\n","Resolving github.com (github.com)... 20.207.73.82\n","Connecting to github.com (github.com)|20.207.73.82|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/25354393/c74e12d7-7166-4aaa-9d7a-bbb01471db75?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240219%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240219T055132Z&X-Amz-Expires=300&X-Amz-Signature=c78c38a8ac2da5dfb5de5104905900e97227ff7d16901ee823ccb2655844e13a&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=25354393&response-content-disposition=attachment%3B%20filename%3Dgeckodriver-v0.34.0-linux64.tar.gz&response-content-type=application%2Foctet-stream [following]\n","--2024-02-19 11:21:33--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/25354393/c74e12d7-7166-4aaa-9d7a-bbb01471db75?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240219%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240219T055132Z&X-Amz-Expires=300&X-Amz-Signature=c78c38a8ac2da5dfb5de5104905900e97227ff7d16901ee823ccb2655844e13a&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=25354393&response-content-disposition=attachment%3B%20filename%3Dgeckodriver-v0.34.0-linux64.tar.gz&response-content-type=application%2Foctet-stream\n","Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n","Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.110.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 3249164 (3.1M) [application/octet-stream]\n","Saving to: ‘geckodriver-v0.34.0-linux64.tar.gz’\n","\n","geckodriver-v0.34.0 100%[===================>]   3.10M  5.98MB/s    in 0.5s    \n","\n","2024-02-19 11:21:34 (5.98 MB/s) - ‘geckodriver-v0.34.0-linux64.tar.gz’ saved [3249164/3249164]\n","\n","geckodriver\n","mv: cannot move 'geckodriver' to '/usr/local/bin/geckodriver': Permission denied\n","Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: selenium in /home/keshu/.local/lib/python3.10/site-packages (4.17.2)\n","Requirement already satisfied: trio-websocket~=0.9 in /home/keshu/.local/lib/python3.10/site-packages (from selenium) (0.11.1)\n","Requirement already satisfied: typing_extensions>=4.9.0 in /home/keshu/.local/lib/python3.10/site-packages (from selenium) (4.9.0)\n","Requirement already satisfied: certifi>=2021.10.8 in /home/keshu/.local/lib/python3.10/site-packages (from selenium) (2023.11.17)\n","Requirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/lib/python3/dist-packages (from selenium) (1.26.5)\n","Requirement already satisfied: trio~=0.17 in /home/keshu/.local/lib/python3.10/site-packages (from selenium) (0.24.0)\n","Requirement already satisfied: outcome in /home/keshu/.local/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n","Requirement already satisfied: sortedcontainers in /home/keshu/.local/lib/python3.10/site-packages (from trio~=0.17->selenium) (2.4.0)\n","Requirement already satisfied: exceptiongroup in /home/keshu/.local/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.2.0)\n","Requirement already satisfied: attrs>=20.1.0 in /home/keshu/.local/lib/python3.10/site-packages (from trio~=0.17->selenium) (23.2.0)\n","Requirement already satisfied: idna in /usr/lib/python3/dist-packages (from trio~=0.17->selenium) (3.3)\n","Requirement already satisfied: sniffio>=1.3.0 in /home/keshu/.local/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.3.0)\n","Requirement already satisfied: wsproto>=0.14 in /home/keshu/.local/lib/python3.10/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n","Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /home/keshu/.local/lib/python3.10/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n","Requirement already satisfied: h11<1,>=0.9.0 in /home/keshu/.local/lib/python3.10/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n","Defaulting to user installation because normal site-packages is not writeable\n","Collecting geckodriver\n","  Using cached geckodriver-0.0.1.tar.gz (1.0 kB)\n","  Installing build dependencies ... \u001b[?25ldone\n","\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n","\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n","\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25hDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/c2/f5/22e003ecf00fed7980e4dd1330d6873ed449971ac2febd9a6650d0cd24e5/geckodriver-0.0.1.tar.gz#sha256=58cd79641be83188cd4b53a02a1692bae72ae6eec0901725e992956dde98dbbf (from https://pypi.org/simple/geckodriver/)\u001b[0m: \u001b[33mRequested geckodriver from https://files.pythonhosted.org/packages/c2/f5/22e003ecf00fed7980e4dd1330d6873ed449971ac2febd9a6650d0cd24e5/geckodriver-0.0.1.tar.gz#sha256=58cd79641be83188cd4b53a02a1692bae72ae6eec0901725e992956dde98dbbf has inconsistent version: filename has '0.0.1', but metadata has '1.0.0'\u001b[0m\n","\u001b[31mERROR: Could not find a version that satisfies the requirement geckodriver (from versions: 0.0.1)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for geckodriver\u001b[0m\u001b[31m\n","\u001b[0m"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_14825/2955951613.py:19: DeprecationWarning: \n","Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n","(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n","but was not found to be installed on your system.\n","If this would cause problems for you,\n","please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n","        \n","  import pandas as pd\n"]}],"source":["!apt-get update\n","!apt install -y firefox\n","!wget https://github.com/mozilla/geckodriver/releases/download/v0.34.0/geckodriver-v0.34.0-linux64.tar.gz\n","!tar -xvzf geckodriver-v0.34.0-linux64.tar.gz\n","!mv geckodriver /usr/local/bin/\n","!pip install selenium\n","!pip install geckodriver\n","\n","from selenium import webdriver\n","from selenium.webdriver.common.by import By\n","import csv\n","from datetime import datetime\n","from selenium.webdriver.support.ui import WebDriverWait\n","from selenium.webdriver.support import expected_conditions as EC\n","from selenium.webdriver.common.action_chains import ActionChains\n","from selenium import webdriver\n","import re\n","import numpy as np\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"id":"EYnPo-Q0o24f","metadata":{"id":"EYnPo-Q0o24f","tags":[]},"outputs":[],"source":["#Extracting sanrafael data\n","\n","class SanRafaelDataExtractor:\n","    def __init__(self):\n","        self.options = webdriver.FirefoxOptions()\n","        self.options.add_argument('--headless')\n","        self.driver = webdriver.Firefox(options=self.options)\n","\n","    def extract_data(self):\n","        # Get the webpage\n","        self.driver.get(\"https://www.cityofsanrafael.org/major-planning-projects-2/\")\n","\n","        self.driver.execute_script(\"window.scrollBy(0, 500);\")\n","        sanrafael_table = self.driver.find_element('css selector', 'tbody')\n","        sanrafael_rows = sanrafael_table.find_elements('css selector', \"tr\")\n","        data = []\n","        for row in sanrafael_rows[1:]:\n","            cells = row.find_elements('css selector', \"td\")\n","            row_data = [cell.text for cell in cells]\n","            data.append(row_data)\n","        return data\n","\n","    def save_to_csv(self, data, headers):\n","        with open('sanrafael.csv', 'w', newline='') as csvfile:\n","            csv_writer = csv.writer(csvfile)\n","            csv_writer.writerow(headers)\n","            csv_writer.writerows(data)\n","\n","    def cleanup_data(self, data):\n","        #Create an empty dataframe with the desired columns\n","        sanrafael_df = pd.DataFrame(columns=['original_id','aug_id','country_name','country_code','map_coordinates','url','region_name','region_code','title', 'description', 'status','stages','date','procurementMethod','budget','currency','buyer','sector','subsector'])\n","        sanrafael_csv = pd.DataFrame(data, columns=['title', 'description', 'floor area', 'Number Units', 'BMR Units', 'applicant', 'staff', 'status'])\n","        sanrafael_csv.drop(['floor area', 'Number Units', 'BMR Units', 'applicant', 'staff'], axis=1, inplace=True)\n","        columns = ['title','description','status']\n","        for column in columns:\n","            sanrafael_df[column]=sanrafael_csv[column]\n","        return sanrafael_df\n","\n","    def close_driver(self):\n","        self.driver.quit()\n","\n","if __name__ == \"__main__\":\n","    headers = ['title','description','floor area','Number Units','BMR Units','applicant','staff','status']\n","    extractor = SanRafaelDataExtractor()\n","    data = extractor.extract_data()\n","    extractor.save_to_csv(data, headers)\n","    modified_data = extractor.cleanup_data(data)\n","    modified_data.to_csv('modified_sanrafael.csv', index=False, na_rep='NaN')\n","    extractor.close_driver()\n"]},{"cell_type":"code","execution_count":null,"id":"W449wzApE5a5","metadata":{"id":"W449wzApE5a5","tags":[]},"outputs":[],"source":["# Extracting Fairfield data\n","\n","class FairFieldDataExtractor:\n","  def __init__(self):\n","    self.options=webdriver.FirefoxOptions()\n","    self.options.add_argument(\"--headless\")\n","    self.driver = webdriver.Firefox(options=self.options)\n","\n","  def extract_data(self):\n","    # Get the webpage\n","    self.driver.get(\"https://www.fairfield.ca.gov/government/city-departments/community-development/planning-division/development-activity?locale=en,https://www.fairfield.ca.gov/government/city-departments/public-works/capital-improvement-projects\")\n","    # Close the pop-up\n","    self.driver.find_element('css selector', '.prefix-overlay-close.prefix-overlay-action-later').click()\n","    # Scroll down by 500 pixels\n","    self.driver.execute_script(\"window.scrollBy(0,500)\")\n","    # Find the table using CSS Selector\n","    fairfield_table = self.driver.find_element('css selector', \"tbody\")\n","    # Get all the rows from the table using find element\n","    fairfield_rows = fairfield_table.find_elements('css selector', \"tr\")\n","    fair_data=[]\n","    # Iterate through the rows and extract data\n","    for row in fairfield_rows:\n","      # Get all cells present in the current row\n","      cells = row.find_elements('css selector', \"td\")\n","      # Extract and write data from each cell to csv file\n","      row_data = [cell.text for cell in cells]\n","      fair_data.append(row_data)\n","    return fair_data\n","\n","  def save_to_csv(self,data,headers):\n","    # Create a csv file to store results\n","    with open('fairfield.csv', 'w', newline='') as csvfile:\n","      csv_writer = csv.writer(csvfile)\n","      csv_writer.writerow(headers)\n","      csv_writer.writerows(data)\n","\n","  def cleanup_data(self,data):\n","    #Create an empty dataframe with the desired columns\n","    fairfield_df = pd.DataFrame(columns=['original_id','aug_id','country_name','country_code','map_coordinates','url','region_name','region_code','title', 'description', 'status','stages','date','procurementMethod','budget','currency','buyer','sector','subsector'])\n","    #read the csv file to dataframe\n","    fairfield_csv = pd.read_csv('fairfield.csv')\n","    #drop the unnecessary rows\n","    fairfield_csv.drop([0,1],axis=0,inplace=True)\n","    #mapping columns\n","    columns = ['original_id','title','location','subsector']\n","    for column in columns:\n","      fairfield_df[column]=fairfield_csv[column]\n","    return fairfield_df\n","\n","  def close_driver(self):\n","    self.driver.quit()\n","\n","\n","if __name__ == \"__main__\":\n","    headers = ['original_id','title','location','subsector']\n","    fairfield_extractor = FairFieldDataExtractor()\n","    fairfield_data = fairfield_extractor.extract_data()\n","    fairfield_extractor.save_to_csv(fairfield_data, headers)\n","    fairfield_modified_data = fairfield_extractor.cleanup_data(fairfield_data)\n","    fairfield_modified_data.to_csv('modified_fairfield.csv', index=False, na_rep='NaN')\n","    fairfield_extractor.close_driver()\n"]},{"cell_type":"code","execution_count":null,"id":"mvqr2gJTShCW","metadata":{"id":"mvqr2gJTShCW","tags":[]},"outputs":[],"source":["#extracting elk\n","class ElkGroveDataExtractor:\n","  def __init__(self):\n","    self.options= webdriver.FirefoxOptions()\n","    self.options.add_argument(\"--headless\")\n","    self.driver= webdriver.Firefox(options =self.options)\n","\n","  def extract_data(self):\n","    # Get the webpage\n","    self.driver.get(\"https://www.elkgrovecity.org/southeast-policy-area/development-projects\")\n","    #Get all rows\n","    elk_rows=self.driver.find_elements('css selector',\"tr\")\n","    elk_data = []\n","    for row in elk_rows:\n","      # Get all the cells present in the row\n","      cells = row.find_elements('css selector', \"td\")\n","      # Extract each cell and write the data to csv\n","      row_data = [cell.text for cell in cells]\n","      elk_data.append(row_data)\n","    return elk_data\n","\n","  def save_to_csv(self,data,headers):\n","    with open('elk.csv', 'w', newline='') as csvfile:\n","       csv_writer = csv.writer(csvfile)\n","       self.driver.execute_script(\"window.scrollBy(0,550)\")\n","       csv_writer.writerow(headers)\n","       csv_writer.writerows(data)\n","\n","  def cleanup_data(self,data):\n","    # Create an empty dataframe with the desired columns\n","    elk_df = pd.DataFrame(columns=['original_id','aug_id','country_name','country_code','map_coordinates','url','region_name','region_code','title', 'description', 'status','stages','date','procurementMethod','budget','currency','buyer','sector','subsector'])\n","    #read the csv file to dataframe\n","    elk_csv = pd.read_csv('elk.csv')\n","    #mapping columns\n","    mapping_columns = ['title','description','status']\n","    for column in mapping_columns:\n","      elk_df[column] = elk_csv[column]\n","    return elk_df\n","\n","  def close_driver(self):\n","    self.driver.quit()\n","\n","if __name__ == \"__main__\":\n","  headers = ['project_code','title','description','applicant','status','project_materials']\n","  elkgrover_extractor = ElkGroveDataExtractor()\n","  elkgrover_data = elkgrover_extractor.extract_data()\n","  elkgrover_extractor.save_to_csv(elkgrover_data,headers)\n","  modified_data = elkgrover_extractor.cleanup_data(elkgrover_data)\n","  modified_data.to_csv('modified_elk.csv',index=False,na_rep='NaN')\n","  elkgrover_extractor.close_driver()"]},{"cell_type":"code","execution_count":null,"id":"Vh8DzGqOA7Bg","metadata":{"id":"Vh8DzGqOA7Bg","outputId":"21412d6f-f508-4aa7-fcb3-049c91a2abe2"},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_19353/98782265.py:49: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  arcata_df = arcata_df.replace(r'^\\s*$', np.nan, regex=True)\n"]}],"source":["#eracting arcata data\n","\n","class ArcataDataExtractor:\n","  def __init__(self):\n","      self.options = webdriver.FirefoxOptions()\n","      self.options.add_argument(\"--headless\")\n","      self.driver = webdriver.Firefox(options=self.options)\n","\n","  def extract_data(self):\n","      # Create an empty dataframe with the desired columns\n","      arcata_df = pd.DataFrame(columns=['original_id', 'aug_id', 'country_name', 'country_code', 'map_coordinates', 'url', 'region_name', 'region_code', 'title', 'description', 'status', 'stages', 'date', 'procurementMethod', 'budget', 'currency', 'buyer', 'sector', 'subsector'])\n","      # get the webpage\n","      self.driver.get(\"https://www.cityofarcata.org/413/Current-City-Construction-Projects\")\n","      # find the table with CSS selector\n","      arcata_table = self.driver.find_element('css selector', \"div[class='widgetBody'] table\")\n","      # Get all the rows from the table\n","      arcata_rows = arcata_table.find_elements('css selector', \"tr\")\n","      # Iterate through the row and accumulate title, description, status\n","      for i in range(1, len(arcata_rows)):\n","          # Scroll down the page by 100 pixels\n","          self.driver.execute_script(\"window.scrollBy(0,100)\")\n","          # extract the project title\n","          project_title = self.driver.find_element('css selector', f'tbody tr:nth-child({i}) td:nth-child(1) strong:nth-child(1)')\n","          # extract the description\n","          project_description = self.driver.find_element('css selector', f'tbody tr:nth-child({i}) td:nth-child(2)')\n","          # extract the status\n","          project_status = self.driver.find_element('css selector', f'tbody tr:nth-child({i}) td:nth-child(3)')\n","          # regex pattern to extract budget\n","          budget_pattern = r'\\$(\\d+(?:,\\d{3})*(?:\\.\\d+)?)(?:\\s*(million|billion|thousand))?'\n","          # Search for the pattern in the text\n","          bud_match = re.search(budget_pattern, project_description.text)\n","          if bud_match:\n","              project_budget = bud_match.group(0)\n","          else:\n","              project_budget = \"null\"\n","          # regex pattern to extract buyer\n","          buyer_pattern = r'contracted with\\s+([^\\d.,;]+)\\b'\n","          # Find matches in the text\n","          buy_match = re.search(buyer_pattern, project_description.text)\n","          # Print the matches\n","          if buy_match:\n","              project_buyer = buy_match.group(0)\n","          else:\n","              project_buyer = \"null\"\n","          arcata_df = arcata_df._append({\"title\": project_title.text, \"description\": project_description.text,\n","                                          \"status\": project_status.text, \"budget\": project_budget,\n","                                          \"buyer\": project_buyer}, ignore_index=True)\n","      # Replace empty values with NaN\n","      arcata_df = arcata_df.replace(r'^\\s*$', np.nan, regex=True)\n","      return arcata_df\n","\n","  def close_driver(self):\n","      self.driver.quit()\n","\n","if __name__ == \"__main__\":\n","  arcata_extractor = ArcataDataExtractor()\n","  arcata_data = arcata_extractor.extract_data()\n","  arcata_data.to_csv('arcata.csv', index=False, na_rep='NaN')\n","  arcata_extractor.close_driver()\n"]},{"cell_type":"code","execution_count":null,"id":"CyD5H69T6898","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":520},"id":"CyD5H69T6898","outputId":"63bf2131-267b-4f7d-872c-b51c3a019ceb"},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_14825/2851960632.py:69: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  paso_df = paso_df.replace(r'^\\s*$', np.nan, regex=True)\n"]}],"source":["#extracting paso data\n","\n","class PasoDataExtractor:\n","    def __init__(self):\n","        self.options = webdriver.FirefoxOptions()\n","        self.options.add_argument(\"--headless\")\n","        self.driver = webdriver.Firefox()\n","\n","    def extract_data(self):\n","        # Get the webpage\n","        self.driver.get(\"https://www.prcity.com/363/City-Projects\")\n","        # Create an empty DataFrame\n","        paso_df = pd.DataFrame(columns=['original_id', 'aug_id', 'country_name', 'country_code', 'map_coordinates', 'url', 'region_name', 'region_code', 'title', 'description', 'status', 'stages', 'date', 'procurementMethod', 'budget', 'currency', 'buyer', 'sector', 'subsector'])\n","        # Scroll down the page by 500 pixels\n","        self.driver.execute_script(\"window.scrollBy(0,500)\")\n","        # Click on the 'Out for Bid'\n","        self.driver.find_element('css selector', \".Hyperlink[href='/392/Out-to-Bid']\").click()\n","        self.driver.implicitly_wait(10)\n","        # Scroll down the page by 500 pixels\n","        self.driver.execute_script(\"window.scrollBy(0,500)\")\n","        # Click on Show Me option\n","        self.driver.find_element('css selector', \"#CatID\").click()\n","        # Select 'All Bids' option\n","        self.driver.find_element('css selector', \"option[value='All']\").click()\n","        # Wait until sort by dropdown is visible\n","        sort_dropdown = WebDriverWait(self.driver, 20).until(\n","        EC.visibility_of_element_located((By.ID, \"txtSort\"))\n","        )\n","        # Create action chain object\n","        action = ActionChains(self.driver)\n","        # Click on the sort_dropdown menu\n","        action.move_to_element(sort_dropdown).click().perform()\n","        # Select 'Closing Date' option\n","        self.driver.find_element('css selector', \"option[value='Date']\").click()\n","        # Filter the bids which are in open status\n","        self.driver.find_element('css selector', \"#showAllBids\").click()\n","        # Scroll down the page by 50 pixels\n","        self.driver.execute_script(\"window.scrollBy(0,400)\")\n","        # Find the table using CSS Selector\n","        paso_table = self.driver.find_element('css selector', \".bidItems\")\n","        # Fetch the rows in the table\n","        paso_rows = paso_table.find_elements('css selector', \"tr\")\n","        project_titles = []\n","        #Iterating through all the rows and storing the project title\n","        for row in paso_rows:\n","            links = row.find_elements('css selector', \"a\")\n","            for link in links:\n","                if link.text != \"Read on\":\n","                    project_titles.append(link.text)\n","        for link in project_titles:\n","            self.driver.execute_script(\"window.scrollBy(0,150)\")\n","            xpath = f'//a[contains(text(), \"{link}\")]'\n","            #click on the project title\n","            self.driver.find_element(By.XPATH, xpath).click()\n","            #extract the description details\n","            details=self.driver.find_element('css selector',\"tbody tr p:nth-child(1)\")\n","            #extract the  publication details\n","            publication_details =self.driver.find_element('css selector',\"body > div:nth-child(26) > div:nth-child(1) > div:nth-child(2) > div:nth-child(2) > div:nth-child(1) > div:nth-child(3) > div:nth-child(1) > div:nth-child(1) > div:nth-child(1) > div:nth-child(1) > div:nth-child(2) > div:nth-child(1) > form:nth-child(2) > div:nth-child(12) > table:nth-child(5) > tbody:nth-child(1) > tr:nth-child(1) > td:nth-child(2) > div:nth-child(3) > div:nth-child(3) > table:nth-child(1) > tbody:nth-child(1) > tr:nth-child(1) > td:nth-child(1) > table:nth-child(1) > tbody:nth-child(1) > tr:nth-child(4) > td:nth-child(1) > span:nth-child(1)\")\n","            #parse the publications details to date time object\n","            date_object  = datetime.strptime(publication_details .text, \"%m/%d/%Y %I:%M %p\")\n","            #xtract date\n","            publication_date = date_object.date()\n","            #extract status\n","            status = self.driver.find_element(By.XPATH,'//span[normalize-space()=\"Open\"]')\n","            paso_df = paso_df._append({'title': link, 'description': details.text,'status':status.text, 'date': publication_date}, ignore_index=True)\n","            self.driver.back()\n","\n","        # Replace empty values with NaN\n","        paso_df = paso_df.replace(r'^\\s*$', np.nan, regex=True)\n","        return paso_df\n","\n","    def close_driver(self):\n","      self.driver.quit()\n","\n","if __name__==\"__main__\":\n","  paso_extractor = PasoDataExtractor()\n","  paso_data=paso_extractor.extract_data()\n","  paso_data.to_csv('modified_paso.csv',index=False,na_rep='NaN')\n","  paso_extractor.close_driver()\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":5}